{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_parquet(\"data/ebnerd_large/articles.parquet\")\n",
    "train_hist = pd.read_parquet(\"data/ebnerd_large/train/history.parquet\")\n",
    "train_behaviors = pd.read_parquet(\"data/ebnerd_large/train/behaviors.parquet\")\n",
    "# val_hist = pd.read_parquet(\"data/ebnerd_large/validation/history.parquet\")\n",
    "# val_behaviors = pd.read_parquet(\"data/ebnerd_large/validation/behaviors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.head()#['subcategory'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['ner_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_behaviors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_behaviors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_hist = pd.read_parquet(\"data/ebnerd_large/validation/history.parquet\")\n",
    "val_behaviors = pd.read_parquet(\"data/ebnerd_large/validation/behaviors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_behaviors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_hist = pd.read_parquet(\"data/ebnerd_large/test/history.parquet\")\n",
    "test_behaviors = pd.read_parquet(\"data/ebnerd_large/test/behaviors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_behaviors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/transe_wikidata5m.pkl\", \"rb\") as fin:\n",
    "    model = pickle.load(fin)\n",
    "entity2id = model.graph.entity2id\n",
    "relation2id = model.graph.relation2id\n",
    "entity_embeddings = model.solver.entity_embeddings\n",
    "relation_embeddings = model.solver.relation_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity2id\n",
    "# relation2id\n",
    "# entity_embeddings\n",
    "# relation_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_parquet, to_numeric\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = read_parquet(\"GLORY/data/ebnerd_small/articles.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Processing raw news: 20738it [00:00, 21142.41it/s]\n"
     ]
    }
   ],
   "source": [
    "art_dict = {}\n",
    "\n",
    "for idx, line in tqdm(articles.iterrows(), desc=f\"[Processing raw news\"):\n",
    "    # print(line)\n",
    "    news_id = line['article_id']\n",
    "    title = line['title']\n",
    "\n",
    "    art_dict[news_id] = title\n",
    "\n",
    "    # print(f\"{idx}. {news_id} : {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = read_parquet(\"GLORY/data/ebnerd_small/train/history.parquet\")\n",
    "behaviors = read_parquet(\"GLORY/data/ebnerd_small/train/behaviors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list, user_set = [], set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _,line in tqdm(hist.iterrows()):\n",
    "    # print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_art = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "232887it [02:20, 1659.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for _,line in tqdm(behaviors.iterrows()):\n",
    "    # print(line)\n",
    "    iid = line['impression_id']\n",
    "    uid = line['user_id']\n",
    "    time = line['impression_time']\n",
    "    history = hist[hist['user_id'] == uid]['article_id_fixed'].values[0]\n",
    "    [hist_art.add(h) for h in history]\n",
    "    imp = list(line['article_ids_inview'])\n",
    "    click = line['article_ids_clicked']\n",
    "    click_one_hot = [0] * len(imp)\n",
    "    click_id = [imp.index(c) for c in click]\n",
    "    for c in click_id:\n",
    "        click_one_hot[c] = 1\n",
    "    impressions = list(zip(imp, click_one_hot))\n",
    "    new_line = '\\t'.join([str(iid), str(uid), str(time), ' '.join(str(h) for h in history), ' '.join('-'.join([str(id), str(i)]) for id,i in impressions)]) + '\\n'\n",
    "    lines.append(new_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20738, 8786)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(art_dict), len(hist_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8783, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "n = 0\n",
    "for h in hist_art:\n",
    "    if art_dict[h]:\n",
    "        y += 1\n",
    "    else:\n",
    "        n += 1\n",
    "        # print(f\"{h}\")\n",
    "\n",
    "y, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frygter cyberangreb under Eurovision'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_dict[int('9748432')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['149474\\t139836\\t2023-05-24 07:47:53\\t9745590 9748574 9748432 9748080 9750687 9750802 9750829 9750793 9750726 9748041 4800276 9757533 9760252 9760528 9760272 9760067 9765336 9765185 9765156\\t9778623-0 9778682-0 9778669-0 9778657-1 9778736-0 9778728-0\\n',\n",
       " '150528\\t143471\\t2023-05-24 07:33:25\\t9737881 9738659 9738569 9738490 9738528 9737521 9736689 9738760 9738684 9733713 9736693 9738569 9737393 9738746 9738303 9738902 9739035 9738968 9739088 9739065 9738980 9739042 9735495 9737243 9736662 9739411 9738974 9739499 9739443 9739634 9739072 9739640 9736767 9739864 9739471 9739293 9735311 9739864 9739883 9739553 9740003 9739999 9739967 9735579 9735579 9735579 9740427 9740223 9740013 9740432 9740400 9739938 9740843 9740710 9740662 9740570 9740962 9740957 9741428 9741449 9741124 9741419 9741108 9729924 9737083 9741508 9741476 9741360 9741311 9741415 9741523 9738585 9739272 9741788 9741772 9742146 9742014 9741896 9738844 9740324 9740711 9667501 9742801 9742770 9742735 9735166 9742327 9740576 9743217 9736862 9743090 9743099 9742840 9743768 9743733 9740845 9743714 9743746 9743702 9743207 9739399 9743911 9743894 9743934 9740021 9743947 9737143 9742966 9744477 9744481 9744297 9743321 9740556 9744213 9744574 9744530 9744530 9744786 9743622 9725132 9743992 9744321 9744931 9744925 9744880 9744406 9744922 9744884 9744880 9745298 9745221 9745471 9744049 9745399 9745044 9745402 9744055 9744793 9745148 9745450 9745148 9745769 9745733 9745723 9745671 9744564 9745563 9745532 9738278 9745698 9745741 9742800 9745723 9743860 9746483 9745869 9744341 9746361 9745912 9746068 9744152 9745900 9746648 9746170 9746683 9746558 9743320 9745883 9745929 9746984 9747119 9746984 9747092 9745931 9746979 9746972 9745524 9747220 9747220 9747404 9746105 9747453 7787495 9747480 9745438 9747480 9747437 9747480 9745848 9522697 9747299 9747495 9744229 9747996 9748482 9748519 9748509 9746627 9748447 9748104 9748158 9748038 9748576 9747747 9748532 9745484 9748918 9748614 9748798 9748592 9748582 9746073 9748977 9748976 9747490 9745484 9747959 9748700 9749224 9749092 9749156 9748980 9748916 9750111 9750064 9749857 9749349 9749644 9749275 9727216 9749756 9750397 9749240 9749277 9750002 9750358 9748670 9748670 9745750 9628759 9745750 9749916 9745750 9735795 9745750 9750307 9745750 9750162 9745750 9750161 9745750 9748450 9745750 9745750 9750189 9750090 9751850 9749743 9749743 9750833 9735795 9751665 9751975 9725978 9751705 9752342 9752480 9752416 9749469 9748043 9752244 9752323 9749966 9752338 9752243 9752243 9752905 9752939 9752375 9718298 6480054 9753902 9753902 9752685 9753901 9753545 9754048 9363981 9754520 9754413 9754271 9754374 9754269 9753748 9754484 9753207 9753795 9754814 9754798 9755430 9755327 9755361 9755298 9755088 9753995 9753995 9755269 9755511 9755098 9749849 9755181 9754025 9755730 9756096 9754081 9755964 9755656 9756091 9484176 9756091 9755726 9755999 9755932 9756362 9757676 9757592 9757698 9757622 9756883 9757293 9758025 9758122 9758894 9758594 9758858 9757338 9758074 9757344 9758440 9758882 9758440 9758857 9758440 9759162 9758567 9759142 9757716 9759045 9758312 9759284 9759164 9759201 9759261 9759707 9759398 9759757 9759154 9759700 9759355 9760386 9760048 9759219 9759497 9760137 9760137 9759389 9760067 9760334 9760741 9760689 9760747 9757801 9758182 9759787 9759929 9760399 9754095 9760288 9760944 9760998 9761083 9760112 9761288 9754571 9760271 9761273 9761683 9761363 9761620 9759708 9761862 9763150 9757360 9762952 9762809 9761419 9764025 9763986 9763905 9762373 9763710 9761787 9761419 9764199 9761825 9764070 9763634 9763923 9716537 9763880 9763917 9763702 9763656 9754177 9754265 9762249 9765410 9765450 9765359 9765458 9765641 9765641 9765973 9765895 9766129 9766722 9766627 9754730 9759476 9766464 9767242 9766225 9766909 9767675 9767368 9767675 9767619 9767675 9767302 9767675 9767399 9767675 9767301 9767675 9767908 9767751 9767868 9767507 9766752 9767765 9768722 9767642 9767557 9768062 9767955 9768962 9769135 9768997 9768708 9768962 9766770 9729996 9767637 9769414 9768882 9768566 9769197 9769679 9769572 9769328 9769367 9769684 9770798 9769306 9769650 9770741 9769366 9762288 9770638 9770604 9767697 9770989\\t9778718-0 9778728-0 9778745-0 9778669-0 9778657-0 9778769-0 9778736-0 9778623-1 9778682-0\\n',\n",
       " '153068\\t151570\\t2023-05-24 07:09:04\\t9738303 9738993 9738303 9738902 9738303 9738447 9736706 9738448 9736706 9738760 9735495 9733845 9737393 9733845 9738777 9733845 9736693 9733845 9738533 9733845 9738684 9738355 9738684 9738569 9738684 9738746 9738684 9738746 9739035 9738746 9739036 9739036 9738490 9739036 9739088 9739065 9738646 9739344 9739342 9739362 9738947 9739362 9739179 9739205 9739179 9739333 9739443 9736767 9739485 9736767 9736662 9736767 9739452 9728819 9738660 9737243 9739802 9739844 9739072 9739072 9740710 9740662 9740710 9740662 9740710 9740747 9740710 9740747 9740710 9740710 9740747 9737154 9740427 9740570 9740013 9739864 9740013 9740223 9740332 9739864 9740432 9739864 9740356 9739864 9740332 9740332 9740843 9739938 9740843 9739471 9740405 9739471 9740237 9740749 9740236 9740969 9741057 9740962 9741057 9741508 9741419 9741419 9741449 9741476 9741449 9741428 9741106 9741379 9741106 9733782 9741106 9741421 9741106 9741259 9741415 9738585 9741523 9739272 9741036 9741036 9741165 9741036 9740724 9741802 9741788 9741772 9741788 9738801 9740942 9738801 9741218 9741578 9741218 9740488 9741218 9741415 9740087 9741986 9742039 9741896 9741850 9742014 9741804 9741803 9741288 9741994 9741288 9741288 9740652 9700156 9700156 9742039 9700156 9741830 9742255 9740591 9742188 9742188 9742279 9741794 9741144 9741997 9742509 9741997 9742423 9742190 9742423 9742440 9742423 9738844 9738844 9740711 9738844 9740324 9738844 9742379 9738844 9742479 9742479 9740408 9667501 9742310 9667501 9667501 9742225 9667501 9742625 9742542 9742770 9736862 9742770 9742775 9742635 9742775 9520286 9742775 9742748 9742280 9740553 9742280 9742815 9742885 9742735 9742145 9742793 9735166 9730301 9742982 9730301 9743207 9743602 9743702 9743602 9743574 9743602 9742401 9739399 9740618 9740156 9740618 9742073 9740618 9743195 9743572 9743195 9743320 9743386 9743320 9741848 9743320 9743755 9743768 9743733 9742966 9743779 9740845 9743746 9744477 9744381 9744481 9744114 9744481 9744297 9744263 9744297 9744313 9744297 9744216 9744297 9744193 9744686 9744213 9744595 9744595 9743893 9744595 9744386 9744453 9744492 9742261 9744553 9744884 9744922 9744884 9744738 9744321 9744931 9744321 9744782 9744925 9744530 9744530 9744880 9744530 9743997 9745367 9745221 9705903 9745399 9745402 9745399 9745084 9745399 9737199 9745399 9745298 9743949 9745298 9744055 9745298 9744793 9055548 9744793 9744098 9745015 9744098 9745613 9745500 9745613 9745476 9745471 9745476 9745532 9745419 9746068 9746049 9745698 9745912 9746148 9744152 9745883 9745929 9746170 9746483 9746558 9746482 9746615 9746639 9745869 9746187 9745869 9745511 9746361 9745511 9745511 9745511 9746648 9745898 9745898 9746035 9746870 9746499 9746723 9745909 9746723 9738714 9746855 9746687 9747220 9747119 9744969 9744969 9746395 9746129 9747074 9746697 9747328 9747267 9745706 9747277 9747320 9747277 9747277 9745706 9747270 9745745 9747925 9748038 9745661 9747839 9745661 9746289 9746105 9747762 9746105 9747847 9746105 9746601 9746105 9747526 9747687 9746105 9746430 9746105 9747796 9747796 9747404 9747796 9747781 9747796 9745709 9748532 9748470 9748532 9748470 9748080 9740872 9748482 9740872 9748508 9740872 9747996 9740872 9747641 9740872 9746202 9748323 9745484 9748801 9745484 9748792 9745484 9748798 9748592 9748798 9748614 9492468 9748467 9492468 9748760 9492468 9746376 9748700 9746376 9748706 9750389 9749756 9750389 9750318 9749582 9750358 9749582 9748670 9749916 9748670 9748670 9745750 9750307 9747411 9750527 9750527 9749637 9749240 9750076 9727216 9749886 9750691 9750705 9751115 9751143 9751107 9751123 9750862 9751123 9751064 9751123 9750864 9750864 9750971 9750864 9748248 9750864 9750500 9751146 9750904 9750898 9750904 9750829 9729924 9750304 9749634 9749110 9750696 9749110 9750304 9749110 9751385 9751139 9751367 9751284 9751349 9751202 9751290 9751020 9751290 9751290 9751290 9751095 9751290 9751220 9749154 9751220 9751220 9748750 9751517 9749184 9751508 9751485 9751508 9749034 9751450 9749034 9748750 9751532 9751654 9751564 9751593 9749873 9744480 9751509 9725235 9750995 9751524 9751524 9751557 9751524 9751508 9751524 9750861 9751524 9750833 9750196 9751531 9751705 9751411 9751705 9747369 9751633 9751706 9751633 9751764 9751633 9751778 9751772 9751688 9751717 9749182 9749182 9751895 9749182 9751897 9749182 9749182 9751921 9751901 9750022 9751901 9749743 9752124 9751252 9752155 9751252 9751962 9751252 9752146 9751252 9752063 9751665 9752063 9751646 9752323 9751673 9752299 9751673 9750873 9749938 9749966 9750891 9751267 9750227 9752342 9752416 9752480 9749469 9748043 9749469 9752905 9752939 9752846 9751123 9752846 9747666 9752867 9747666 9752375 9747666 9752788 9747666 9752479 9751671 9752479 9752744 9752479 9752962 9649560 9752962 9753295 9752332 9752998 9752332 9753168 9752332 9752984 9753521 9753503 9753543 9753526 9752463 9753540 9753455 9754484 9754413 9754271 9754520 9754490 9754374 9752566 9754374 9747961 9754269 9753748 9751769 9753748 9753811 9753748 9754294 9753748 9754149 9753748 9754290 9753207 9754159 9753207 9753985 9754603 9754288 9754350 9748186 9754350 9753995 9755119 9753773 9752593 9753773 9753773 9754361 9753774 9755178 9755430 9755511 9663313 9755098 9755505 9755098 9755269 9755361 9755364 9755361 9755327 9755285 9755327 9755088 9755327 9756362 9744301 9756362 9756425 9756362 9756297 9756362 9755964 9756295 9756091 9754081 9754087 9756190 9756879 9753047 9756879 9756925 9756879 9756020 9756683 9756020 9749944 9756020 9756369 9756788 9756287 9756788 9747684 9758122 9757819 9754366 9757541 9757876 9757541 9757866 9757541 9757717 9757541 9757372 9757891 9757372 9758103 9757372 9755749 9758318 9758347 9758318 9758326 9757426 9758252 9748213 9758538 9745619 9758538 9758025 9758025 9754121 9758544 9758483 9758544 9758544 9758544 9757428 9755580 9758561 9757344 9757338 9757344 9758074 9757344 9758453 9757344 9758705 9759026 9759045 9759026 9758894 9759026 9759026 9758765 9759025 9759045 9758599 9758857 9758599 9758858 9760067 9760334 9760477 9760288 9760364 9760288 9760221 9760288 9759957 9759301 9760386 9759508 9760222 9759508 9759497 9759497 9758328 9759497 9759398 9760271 9761078 9760271 9760112 9761083 9761087 9761273 9761255 9761273 9761047 9761273 9760944 9761273 9760962 9761273 9760962 9761273 9761127 9759782 9760446 9759782 9760829 9761914 9762067 9761862 9761861 9761771 9761861 9761803 9761803 9761599 9761803 9761697 9761803 9761858 9761803 9761359 9760290 9761359 9761842 9761359 9761531 9761359 9761620 9761359 9761683 9761359 9760796 9761359 9760735 9761359 9762377 9760900 9762718 9761618 9762407 9708367 9762407 9763120 9757686 9762809 9757686 9761419 9762377 9763247 9761588 9763279 9762548 9763090 9763291 9762953 9763202 9761782 9763113 9763634 9763579 9763656 9763613 9763656 9763554 9763656 9763596 9763656 9763448 9763656 9763942 9763857 9763721 9763857 9763880 9763857 9763710 9763710 9763917 9763882 9763882 9764086 9764049 9763905 9764079 9761801 9763949 9764325 9764325 9758057 9735278 9763854 9764070 9763854 9764433 9764443 9764403 9763559 9764362 9763445 9764362 9764420 9760390 9760390 9765450 9765410 9765410 9765410 9765336 9764641 9765336 9765185 9765244 9765067 9765244 9765194 9765244 9765156 9764433 9765545 9765551 9765218 9765551 9765153 9765582 9765153 9765481 9765153 9759612 9763400 9766042 9765641 9766419 9765641 9765803 9766382 9765803 9766279 9766338 9766279 9766379 9765943 9724690 9765943 9766348 9762071 9765925 9762071 9765010 9766722 9754730 9766627 9736876 9766627 9765846 9766627 9766627 9759476 9766428 9759476 9766634 9759476 9766620 9759476 9766048 9766635 9766261 9766635 9765907 9765907 9729988 9766757 9765965 9754042 9765965 9766770 9766770 9767220 9767027 9767071 9766242 9766949 9766944 9766949 9766886 9762678 9766886 9767955 9757226 9768387 9757226 9768515 9757226 9768515 9757226 9768377 9767557 9767602 9768599 9767722 9768469 9768802 9768820 9768802 9768819 9768802 9767765 9768722 9767765 9768708 9767765 9768790 9767765 9760758 9767765 9768685 9769135 9767604 9768962 9767534 9768962 9768564 9768583 9768564 9768829 9762135 9768722 9769641 9769450 9769641 9770194 9769641 9770207 9769641 9770030 9770218 9770218 9769622 9770146 9770178 9770425 9770178 9769888 9769888 9767868 9748104 9769220 9768260 9762288 9768260 9769580 9770638 9769580 9770604 9769580 9770620 9770594 9770726 9769404 9770400 9770799 9770400 9762288 9770400 9769366 9770400 9770741 9770798 9769306 9769650 9769457 9769575 9769457 9769457 9767697 9735909 9767697 9770538 9770989 9770538 9770538 9769553 9770538 9770541 9770829\\t9778657-0 9778669-1 9772866-0 9776259-0 9756397-0 9693002-0 9778682-0\\n',\n",
       " '153070\\t151570\\t2023-05-24 07:13:14\\t9738303 9738993 9738303 9738902 9738303 9738447 9736706 9738448 9736706 9738760 9735495 9733845 9737393 9733845 9738777 9733845 9736693 9733845 9738533 9733845 9738684 9738355 9738684 9738569 9738684 9738746 9738684 9738746 9739035 9738746 9739036 9739036 9738490 9739036 9739088 9739065 9738646 9739344 9739342 9739362 9738947 9739362 9739179 9739205 9739179 9739333 9739443 9736767 9739485 9736767 9736662 9736767 9739452 9728819 9738660 9737243 9739802 9739844 9739072 9739072 9740710 9740662 9740710 9740662 9740710 9740747 9740710 9740747 9740710 9740710 9740747 9737154 9740427 9740570 9740013 9739864 9740013 9740223 9740332 9739864 9740432 9739864 9740356 9739864 9740332 9740332 9740843 9739938 9740843 9739471 9740405 9739471 9740237 9740749 9740236 9740969 9741057 9740962 9741057 9741508 9741419 9741419 9741449 9741476 9741449 9741428 9741106 9741379 9741106 9733782 9741106 9741421 9741106 9741259 9741415 9738585 9741523 9739272 9741036 9741036 9741165 9741036 9740724 9741802 9741788 9741772 9741788 9738801 9740942 9738801 9741218 9741578 9741218 9740488 9741218 9741415 9740087 9741986 9742039 9741896 9741850 9742014 9741804 9741803 9741288 9741994 9741288 9741288 9740652 9700156 9700156 9742039 9700156 9741830 9742255 9740591 9742188 9742188 9742279 9741794 9741144 9741997 9742509 9741997 9742423 9742190 9742423 9742440 9742423 9738844 9738844 9740711 9738844 9740324 9738844 9742379 9738844 9742479 9742479 9740408 9667501 9742310 9667501 9667501 9742225 9667501 9742625 9742542 9742770 9736862 9742770 9742775 9742635 9742775 9520286 9742775 9742748 9742280 9740553 9742280 9742815 9742885 9742735 9742145 9742793 9735166 9730301 9742982 9730301 9743207 9743602 9743702 9743602 9743574 9743602 9742401 9739399 9740618 9740156 9740618 9742073 9740618 9743195 9743572 9743195 9743320 9743386 9743320 9741848 9743320 9743755 9743768 9743733 9742966 9743779 9740845 9743746 9744477 9744381 9744481 9744114 9744481 9744297 9744263 9744297 9744313 9744297 9744216 9744297 9744193 9744686 9744213 9744595 9744595 9743893 9744595 9744386 9744453 9744492 9742261 9744553 9744884 9744922 9744884 9744738 9744321 9744931 9744321 9744782 9744925 9744530 9744530 9744880 9744530 9743997 9745367 9745221 9705903 9745399 9745402 9745399 9745084 9745399 9737199 9745399 9745298 9743949 9745298 9744055 9745298 9744793 9055548 9744793 9744098 9745015 9744098 9745613 9745500 9745613 9745476 9745471 9745476 9745532 9745419 9746068 9746049 9745698 9745912 9746148 9744152 9745883 9745929 9746170 9746483 9746558 9746482 9746615 9746639 9745869 9746187 9745869 9745511 9746361 9745511 9745511 9745511 9746648 9745898 9745898 9746035 9746870 9746499 9746723 9745909 9746723 9738714 9746855 9746687 9747220 9747119 9744969 9744969 9746395 9746129 9747074 9746697 9747328 9747267 9745706 9747277 9747320 9747277 9747277 9745706 9747270 9745745 9747925 9748038 9745661 9747839 9745661 9746289 9746105 9747762 9746105 9747847 9746105 9746601 9746105 9747526 9747687 9746105 9746430 9746105 9747796 9747796 9747404 9747796 9747781 9747796 9745709 9748532 9748470 9748532 9748470 9748080 9740872 9748482 9740872 9748508 9740872 9747996 9740872 9747641 9740872 9746202 9748323 9745484 9748801 9745484 9748792 9745484 9748798 9748592 9748798 9748614 9492468 9748467 9492468 9748760 9492468 9746376 9748700 9746376 9748706 9750389 9749756 9750389 9750318 9749582 9750358 9749582 9748670 9749916 9748670 9748670 9745750 9750307 9747411 9750527 9750527 9749637 9749240 9750076 9727216 9749886 9750691 9750705 9751115 9751143 9751107 9751123 9750862 9751123 9751064 9751123 9750864 9750864 9750971 9750864 9748248 9750864 9750500 9751146 9750904 9750898 9750904 9750829 9729924 9750304 9749634 9749110 9750696 9749110 9750304 9749110 9751385 9751139 9751367 9751284 9751349 9751202 9751290 9751020 9751290 9751290 9751290 9751095 9751290 9751220 9749154 9751220 9751220 9748750 9751517 9749184 9751508 9751485 9751508 9749034 9751450 9749034 9748750 9751532 9751654 9751564 9751593 9749873 9744480 9751509 9725235 9750995 9751524 9751524 9751557 9751524 9751508 9751524 9750861 9751524 9750833 9750196 9751531 9751705 9751411 9751705 9747369 9751633 9751706 9751633 9751764 9751633 9751778 9751772 9751688 9751717 9749182 9749182 9751895 9749182 9751897 9749182 9749182 9751921 9751901 9750022 9751901 9749743 9752124 9751252 9752155 9751252 9751962 9751252 9752146 9751252 9752063 9751665 9752063 9751646 9752323 9751673 9752299 9751673 9750873 9749938 9749966 9750891 9751267 9750227 9752342 9752416 9752480 9749469 9748043 9749469 9752905 9752939 9752846 9751123 9752846 9747666 9752867 9747666 9752375 9747666 9752788 9747666 9752479 9751671 9752479 9752744 9752479 9752962 9649560 9752962 9753295 9752332 9752998 9752332 9753168 9752332 9752984 9753521 9753503 9753543 9753526 9752463 9753540 9753455 9754484 9754413 9754271 9754520 9754490 9754374 9752566 9754374 9747961 9754269 9753748 9751769 9753748 9753811 9753748 9754294 9753748 9754149 9753748 9754290 9753207 9754159 9753207 9753985 9754603 9754288 9754350 9748186 9754350 9753995 9755119 9753773 9752593 9753773 9753773 9754361 9753774 9755178 9755430 9755511 9663313 9755098 9755505 9755098 9755269 9755361 9755364 9755361 9755327 9755285 9755327 9755088 9755327 9756362 9744301 9756362 9756425 9756362 9756297 9756362 9755964 9756295 9756091 9754081 9754087 9756190 9756879 9753047 9756879 9756925 9756879 9756020 9756683 9756020 9749944 9756020 9756369 9756788 9756287 9756788 9747684 9758122 9757819 9754366 9757541 9757876 9757541 9757866 9757541 9757717 9757541 9757372 9757891 9757372 9758103 9757372 9755749 9758318 9758347 9758318 9758326 9757426 9758252 9748213 9758538 9745619 9758538 9758025 9758025 9754121 9758544 9758483 9758544 9758544 9758544 9757428 9755580 9758561 9757344 9757338 9757344 9758074 9757344 9758453 9757344 9758705 9759026 9759045 9759026 9758894 9759026 9759026 9758765 9759025 9759045 9758599 9758857 9758599 9758858 9760067 9760334 9760477 9760288 9760364 9760288 9760221 9760288 9759957 9759301 9760386 9759508 9760222 9759508 9759497 9759497 9758328 9759497 9759398 9760271 9761078 9760271 9760112 9761083 9761087 9761273 9761255 9761273 9761047 9761273 9760944 9761273 9760962 9761273 9760962 9761273 9761127 9759782 9760446 9759782 9760829 9761914 9762067 9761862 9761861 9761771 9761861 9761803 9761803 9761599 9761803 9761697 9761803 9761858 9761803 9761359 9760290 9761359 9761842 9761359 9761531 9761359 9761620 9761359 9761683 9761359 9760796 9761359 9760735 9761359 9762377 9760900 9762718 9761618 9762407 9708367 9762407 9763120 9757686 9762809 9757686 9761419 9762377 9763247 9761588 9763279 9762548 9763090 9763291 9762953 9763202 9761782 9763113 9763634 9763579 9763656 9763613 9763656 9763554 9763656 9763596 9763656 9763448 9763656 9763942 9763857 9763721 9763857 9763880 9763857 9763710 9763710 9763917 9763882 9763882 9764086 9764049 9763905 9764079 9761801 9763949 9764325 9764325 9758057 9735278 9763854 9764070 9763854 9764433 9764443 9764403 9763559 9764362 9763445 9764362 9764420 9760390 9760390 9765450 9765410 9765410 9765410 9765336 9764641 9765336 9765185 9765244 9765067 9765244 9765194 9765244 9765156 9764433 9765545 9765551 9765218 9765551 9765153 9765582 9765153 9765481 9765153 9759612 9763400 9766042 9765641 9766419 9765641 9765803 9766382 9765803 9766279 9766338 9766279 9766379 9765943 9724690 9765943 9766348 9762071 9765925 9762071 9765010 9766722 9754730 9766627 9736876 9766627 9765846 9766627 9766627 9759476 9766428 9759476 9766634 9759476 9766620 9759476 9766048 9766635 9766261 9766635 9765907 9765907 9729988 9766757 9765965 9754042 9765965 9766770 9766770 9767220 9767027 9767071 9766242 9766949 9766944 9766949 9766886 9762678 9766886 9767955 9757226 9768387 9757226 9768515 9757226 9768515 9757226 9768377 9767557 9767602 9768599 9767722 9768469 9768802 9768820 9768802 9768819 9768802 9767765 9768722 9767765 9768708 9767765 9768790 9767765 9760758 9767765 9768685 9769135 9767604 9768962 9767534 9768962 9768564 9768583 9768564 9768829 9762135 9768722 9769641 9769450 9769641 9770194 9769641 9770207 9769641 9770030 9770218 9770218 9769622 9770146 9770178 9770425 9770178 9769888 9769888 9767868 9748104 9769220 9768260 9762288 9768260 9769580 9770638 9769580 9770604 9769580 9770620 9770594 9770726 9769404 9770400 9770799 9770400 9762288 9770400 9769366 9770400 9770741 9770798 9769306 9769650 9769457 9769575 9769457 9769457 9767697 9735909 9767697 9770538 9770989 9770538 9770538 9769553 9770538 9770541 9770829\\t9020783-0 9778444-0 9525589-0 7213923-0 9777397-0 9778718-0 9430567-0 9778628-1\\n',\n",
       " '153071\\t151570\\t2023-05-24 07:11:08\\t9738303 9738993 9738303 9738902 9738303 9738447 9736706 9738448 9736706 9738760 9735495 9733845 9737393 9733845 9738777 9733845 9736693 9733845 9738533 9733845 9738684 9738355 9738684 9738569 9738684 9738746 9738684 9738746 9739035 9738746 9739036 9739036 9738490 9739036 9739088 9739065 9738646 9739344 9739342 9739362 9738947 9739362 9739179 9739205 9739179 9739333 9739443 9736767 9739485 9736767 9736662 9736767 9739452 9728819 9738660 9737243 9739802 9739844 9739072 9739072 9740710 9740662 9740710 9740662 9740710 9740747 9740710 9740747 9740710 9740710 9740747 9737154 9740427 9740570 9740013 9739864 9740013 9740223 9740332 9739864 9740432 9739864 9740356 9739864 9740332 9740332 9740843 9739938 9740843 9739471 9740405 9739471 9740237 9740749 9740236 9740969 9741057 9740962 9741057 9741508 9741419 9741419 9741449 9741476 9741449 9741428 9741106 9741379 9741106 9733782 9741106 9741421 9741106 9741259 9741415 9738585 9741523 9739272 9741036 9741036 9741165 9741036 9740724 9741802 9741788 9741772 9741788 9738801 9740942 9738801 9741218 9741578 9741218 9740488 9741218 9741415 9740087 9741986 9742039 9741896 9741850 9742014 9741804 9741803 9741288 9741994 9741288 9741288 9740652 9700156 9700156 9742039 9700156 9741830 9742255 9740591 9742188 9742188 9742279 9741794 9741144 9741997 9742509 9741997 9742423 9742190 9742423 9742440 9742423 9738844 9738844 9740711 9738844 9740324 9738844 9742379 9738844 9742479 9742479 9740408 9667501 9742310 9667501 9667501 9742225 9667501 9742625 9742542 9742770 9736862 9742770 9742775 9742635 9742775 9520286 9742775 9742748 9742280 9740553 9742280 9742815 9742885 9742735 9742145 9742793 9735166 9730301 9742982 9730301 9743207 9743602 9743702 9743602 9743574 9743602 9742401 9739399 9740618 9740156 9740618 9742073 9740618 9743195 9743572 9743195 9743320 9743386 9743320 9741848 9743320 9743755 9743768 9743733 9742966 9743779 9740845 9743746 9744477 9744381 9744481 9744114 9744481 9744297 9744263 9744297 9744313 9744297 9744216 9744297 9744193 9744686 9744213 9744595 9744595 9743893 9744595 9744386 9744453 9744492 9742261 9744553 9744884 9744922 9744884 9744738 9744321 9744931 9744321 9744782 9744925 9744530 9744530 9744880 9744530 9743997 9745367 9745221 9705903 9745399 9745402 9745399 9745084 9745399 9737199 9745399 9745298 9743949 9745298 9744055 9745298 9744793 9055548 9744793 9744098 9745015 9744098 9745613 9745500 9745613 9745476 9745471 9745476 9745532 9745419 9746068 9746049 9745698 9745912 9746148 9744152 9745883 9745929 9746170 9746483 9746558 9746482 9746615 9746639 9745869 9746187 9745869 9745511 9746361 9745511 9745511 9745511 9746648 9745898 9745898 9746035 9746870 9746499 9746723 9745909 9746723 9738714 9746855 9746687 9747220 9747119 9744969 9744969 9746395 9746129 9747074 9746697 9747328 9747267 9745706 9747277 9747320 9747277 9747277 9745706 9747270 9745745 9747925 9748038 9745661 9747839 9745661 9746289 9746105 9747762 9746105 9747847 9746105 9746601 9746105 9747526 9747687 9746105 9746430 9746105 9747796 9747796 9747404 9747796 9747781 9747796 9745709 9748532 9748470 9748532 9748470 9748080 9740872 9748482 9740872 9748508 9740872 9747996 9740872 9747641 9740872 9746202 9748323 9745484 9748801 9745484 9748792 9745484 9748798 9748592 9748798 9748614 9492468 9748467 9492468 9748760 9492468 9746376 9748700 9746376 9748706 9750389 9749756 9750389 9750318 9749582 9750358 9749582 9748670 9749916 9748670 9748670 9745750 9750307 9747411 9750527 9750527 9749637 9749240 9750076 9727216 9749886 9750691 9750705 9751115 9751143 9751107 9751123 9750862 9751123 9751064 9751123 9750864 9750864 9750971 9750864 9748248 9750864 9750500 9751146 9750904 9750898 9750904 9750829 9729924 9750304 9749634 9749110 9750696 9749110 9750304 9749110 9751385 9751139 9751367 9751284 9751349 9751202 9751290 9751020 9751290 9751290 9751290 9751095 9751290 9751220 9749154 9751220 9751220 9748750 9751517 9749184 9751508 9751485 9751508 9749034 9751450 9749034 9748750 9751532 9751654 9751564 9751593 9749873 9744480 9751509 9725235 9750995 9751524 9751524 9751557 9751524 9751508 9751524 9750861 9751524 9750833 9750196 9751531 9751705 9751411 9751705 9747369 9751633 9751706 9751633 9751764 9751633 9751778 9751772 9751688 9751717 9749182 9749182 9751895 9749182 9751897 9749182 9749182 9751921 9751901 9750022 9751901 9749743 9752124 9751252 9752155 9751252 9751962 9751252 9752146 9751252 9752063 9751665 9752063 9751646 9752323 9751673 9752299 9751673 9750873 9749938 9749966 9750891 9751267 9750227 9752342 9752416 9752480 9749469 9748043 9749469 9752905 9752939 9752846 9751123 9752846 9747666 9752867 9747666 9752375 9747666 9752788 9747666 9752479 9751671 9752479 9752744 9752479 9752962 9649560 9752962 9753295 9752332 9752998 9752332 9753168 9752332 9752984 9753521 9753503 9753543 9753526 9752463 9753540 9753455 9754484 9754413 9754271 9754520 9754490 9754374 9752566 9754374 9747961 9754269 9753748 9751769 9753748 9753811 9753748 9754294 9753748 9754149 9753748 9754290 9753207 9754159 9753207 9753985 9754603 9754288 9754350 9748186 9754350 9753995 9755119 9753773 9752593 9753773 9753773 9754361 9753774 9755178 9755430 9755511 9663313 9755098 9755505 9755098 9755269 9755361 9755364 9755361 9755327 9755285 9755327 9755088 9755327 9756362 9744301 9756362 9756425 9756362 9756297 9756362 9755964 9756295 9756091 9754081 9754087 9756190 9756879 9753047 9756879 9756925 9756879 9756020 9756683 9756020 9749944 9756020 9756369 9756788 9756287 9756788 9747684 9758122 9757819 9754366 9757541 9757876 9757541 9757866 9757541 9757717 9757541 9757372 9757891 9757372 9758103 9757372 9755749 9758318 9758347 9758318 9758326 9757426 9758252 9748213 9758538 9745619 9758538 9758025 9758025 9754121 9758544 9758483 9758544 9758544 9758544 9757428 9755580 9758561 9757344 9757338 9757344 9758074 9757344 9758453 9757344 9758705 9759026 9759045 9759026 9758894 9759026 9759026 9758765 9759025 9759045 9758599 9758857 9758599 9758858 9760067 9760334 9760477 9760288 9760364 9760288 9760221 9760288 9759957 9759301 9760386 9759508 9760222 9759508 9759497 9759497 9758328 9759497 9759398 9760271 9761078 9760271 9760112 9761083 9761087 9761273 9761255 9761273 9761047 9761273 9760944 9761273 9760962 9761273 9760962 9761273 9761127 9759782 9760446 9759782 9760829 9761914 9762067 9761862 9761861 9761771 9761861 9761803 9761803 9761599 9761803 9761697 9761803 9761858 9761803 9761359 9760290 9761359 9761842 9761359 9761531 9761359 9761620 9761359 9761683 9761359 9760796 9761359 9760735 9761359 9762377 9760900 9762718 9761618 9762407 9708367 9762407 9763120 9757686 9762809 9757686 9761419 9762377 9763247 9761588 9763279 9762548 9763090 9763291 9762953 9763202 9761782 9763113 9763634 9763579 9763656 9763613 9763656 9763554 9763656 9763596 9763656 9763448 9763656 9763942 9763857 9763721 9763857 9763880 9763857 9763710 9763710 9763917 9763882 9763882 9764086 9764049 9763905 9764079 9761801 9763949 9764325 9764325 9758057 9735278 9763854 9764070 9763854 9764433 9764443 9764403 9763559 9764362 9763445 9764362 9764420 9760390 9760390 9765450 9765410 9765410 9765410 9765336 9764641 9765336 9765185 9765244 9765067 9765244 9765194 9765244 9765156 9764433 9765545 9765551 9765218 9765551 9765153 9765582 9765153 9765481 9765153 9759612 9763400 9766042 9765641 9766419 9765641 9765803 9766382 9765803 9766279 9766338 9766279 9766379 9765943 9724690 9765943 9766348 9762071 9765925 9762071 9765010 9766722 9754730 9766627 9736876 9766627 9765846 9766627 9766627 9759476 9766428 9759476 9766634 9759476 9766620 9759476 9766048 9766635 9766261 9766635 9765907 9765907 9729988 9766757 9765965 9754042 9765965 9766770 9766770 9767220 9767027 9767071 9766242 9766949 9766944 9766949 9766886 9762678 9766886 9767955 9757226 9768387 9757226 9768515 9757226 9768515 9757226 9768377 9767557 9767602 9768599 9767722 9768469 9768802 9768820 9768802 9768819 9768802 9767765 9768722 9767765 9768708 9767765 9768790 9767765 9760758 9767765 9768685 9769135 9767604 9768962 9767534 9768962 9768564 9768583 9768564 9768829 9762135 9768722 9769641 9769450 9769641 9770194 9769641 9770207 9769641 9770030 9770218 9770218 9769622 9770146 9770178 9770425 9770178 9769888 9769888 9767868 9748104 9769220 9768260 9762288 9768260 9769580 9770638 9769580 9770604 9769580 9770620 9770594 9770726 9769404 9770400 9770799 9770400 9762288 9770400 9769366 9770400 9770741 9770798 9769306 9769650 9769457 9769575 9769457 9769457 9767697 9735909 9767697 9770538 9770989 9770538 9770538 9769553 9770538 9770541 9770829\\t9777492-1 9774568-0 9565836-0 9335113-0 9771223-0 9131971-0 9778623-0 9770218-0 9775990-0\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing behaviors news to News Graph: 100%|██████████| 232887/232887 [00:01<00:00, 135442.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2426247, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "n = 0\n",
    "\n",
    "for l in tqdm(lines, desc=f\"Processing behaviors news to News Graph\"):\n",
    "    # print(\"lines\", line)\n",
    "    line = l.strip().split('\\t')\n",
    "\n",
    "    # check duplicate user\n",
    "    used_id = line[1]\n",
    "    if used_id in user_set:\n",
    "        continue\n",
    "    else:\n",
    "        user_set.add(used_id)\n",
    "\n",
    "    \n",
    "    # record cnt & read path\n",
    "    history = line[3].split()\n",
    "    if len(history) > 1:\n",
    "        # print(\"history1\", history)\n",
    "        for news_id in history:\n",
    "            if int(news_id) in art_dict:\n",
    "                y += 1\n",
    "                # print(f'yes: {news_id}')\n",
    "            else:\n",
    "                n += 1\n",
    "                # print(f'no: {news_id}')\n",
    "        # long_edge = [art_dict[news_id] for news_id in history]\n",
    "        # print(long_edge)\n",
    "        # edge_list.append(long_edge)\n",
    "\n",
    "    # print(\"history2\", history)\n",
    "\n",
    "y, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataload'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LambdaLR\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_load\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_preprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prepare_preprocessed_data\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataload'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Testing\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.cuda import amp\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from dataload.data_load import load_data\n",
    "from dataload.data_preprocess import prepare_preprocessed_data\n",
    "from utils.metrics import *\n",
    "from utils.common import *\n",
    "\n",
    "### custom your wandb setting here ###\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "# os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"4fbf74f08e1faaadbc3bcd8d184d16f04338332b\"\n",
    "\n",
    "def train(model, optimizer, scaler, scheduler, dataloader, local_rank, cfg, early_stopping):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    sum_loss = torch.zeros(1).to(local_rank)\n",
    "    sum_auc = torch.zeros(1).to(local_rank)\n",
    "    \n",
    "    train_dataset_len = len(dataloader)\n",
    "    print('train : len(dataloader)', len(dataloader))\n",
    "    for cnt, batch in enumerate(dataloader, start=1):\n",
    "        print(f\"Batch {cnt} received from dataloader\")\n",
    "        if cnt == 1:\n",
    "            break\n",
    "    print(\"Dataloader check completed.\")\n",
    "\n",
    "    res = val(model, local_rank, cfg)\n",
    "    model.train()\n",
    "\n",
    "    if local_rank == 0:\n",
    "        save_model(cfg, model, optimizer, f\"{cfg.ml_label}_auc_best\")\n",
    "        wandb.run.summary.update({\"best_auc\": res[\"auc\"],\"best_mrr\":res['mrr'], \n",
    "                            \"best_ndcg5\":res['ndcg5'], \"best_ndcg10\":res['ndcg10']})\n",
    "\n",
    "        pretty_print(res)\n",
    "        wandb.log(res)\n",
    "\n",
    "\n",
    "    # for cnt, (subgraph, mapping_idx, candidate_news, candidate_entity, entity_mask, labels) \\\n",
    "    #     in enumerate(tqdm(dataloader, total=int(train_dataset_len), desc=f\"[{local_rank}] Training\"), start=1):\n",
    "    #     # print(f\"Batch {cnt} - Data received from dataloader\")\n",
    "\n",
    "    #     subgraph = subgraph.to(local_rank, non_blocking=True)\n",
    "    #     mapping_idx = mapping_idx.to(local_rank, non_blocking=True)\n",
    "    #     candidate_news = candidate_news.to(local_rank, non_blocking=True)\n",
    "    #     labels = labels.to(local_rank, non_blocking=True)\n",
    "    #     candidate_entity = candidate_entity.to(local_rank, non_blocking=True)\n",
    "    #     entity_mask = entity_mask.to(local_rank, non_blocking=True)\n",
    "\n",
    "    #     with amp.autocast():\n",
    "    #         bz_loss, y_hat = model(subgraph, mapping_idx, candidate_news, candidate_entity, entity_mask, labels)\n",
    "\n",
    "            \n",
    "    #     # Accumulate the gradients\n",
    "    #     scaler.scale(bz_loss).backward()\n",
    "    #     if cnt % cfg.accumulation_steps == 0 or cnt == int(train_dataset_len / cfg.batch_size):\n",
    "    #         # Update the parameters\n",
    "    #         scaler.step(optimizer)\n",
    "    #         old_scaler = scaler.get_scale()\n",
    "    #         scaler.update()\n",
    "    #         new_scaler = scaler.get_scale()\n",
    "    #         if new_scaler >= old_scaler:\n",
    "    #             scheduler.step()\n",
    "    #             ## https://discuss.pytorch.org/t/userwarning-detected-call-of-lr-scheduler-step-before-optimizer-step/164814\n",
    "    #         optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    #     sum_loss += bz_loss.data.float()\n",
    "    #     sum_auc += area_under_curve(labels, y_hat)\n",
    "    #     # ---------------------------------------- Training Log\n",
    "    #     if cnt % cfg.log_steps == 0:\n",
    "    #         if local_rank == 0:\n",
    "    #             wandb.log({\"train_loss\": sum_loss.item() / cfg.log_steps, \"train_auc\": sum_auc.item() / cfg.log_steps})\n",
    "    #         print('[{}] Ed: {}, average_loss: {:.5f}, average_acc: {:.5f}'.format(\n",
    "    #             local_rank, cnt * cfg.batch_size, sum_loss.item() / cfg.log_steps, sum_auc.item() / cfg.log_steps))\n",
    "    #         sum_loss.zero_()\n",
    "    #         sum_auc.zero_()\n",
    "        \n",
    "    #     if cnt % cfg.val_steps == 0:\n",
    "    #         res = val(model, local_rank, cfg)\n",
    "    #         model.train()\n",
    "\n",
    "    #         if local_rank == 0:\n",
    "    #             save_model(cfg, model, optimizer, f\"{cfg.ml_label}_auc_best\")\n",
    "    #             wandb.run.summary.update({\"best_auc\": res[\"auc\"],\"best_mrr\":res['mrr'], \n",
    "    #                                 \"best_ndcg5\":res['ndcg5'], \"best_ndcg10\":res['ndcg10']})\n",
    "\n",
    "    #             pretty_print(res)\n",
    "    #             wandb.log(res)\n",
    "\n",
    "    #         early_stop, get_better = early_stopping(res['auc'])\n",
    "    #         if early_stop:\n",
    "    #             print(\"Early Stop.\")\n",
    "    #             break\n",
    "    #         elif get_better:\n",
    "    #             print(f\"Better Result!\")\n",
    "    #             if local_rank == 0:\n",
    "    #                 save_model(cfg, model, optimizer, f\"{cfg.ml_label}_auc{res['auc']}\")\n",
    "    #                 wandb.run.summary.update({\"best_auc\": res[\"auc\"],\"best_mrr\":res['mrr'], \n",
    "    #                                      \"best_ndcg5\":res['ndcg5'], \"best_ndcg10\":res['ndcg10']})\n",
    "\n",
    "\n",
    "\n",
    "def val(model, local_rank, cfg):\n",
    "    model.eval()\n",
    "    dataloader = load_data(cfg, mode='val', model=model, local_rank=local_rank)\n",
    "    val_dataset_len = len(dataloader)\n",
    "    print('val : len(dataloader)', len(dataloader))\n",
    "    tasks = []\n",
    "    print('gpus', cfg.gpu_num)\n",
    "    with torch.no_grad():\n",
    "        for cnt, (subgraph, mappings, clicked_entity, candidate_input, candidate_entity, entity_mask, labels) \\\n",
    "                in enumerate(tqdm(dataloader,\n",
    "                                  total=int(val_dataset_len),\n",
    "                                  desc=f\"[{local_rank}] Validating\")):\n",
    "            candidate_emb = torch.FloatTensor(np.array(candidate_input)).to(local_rank, non_blocking=True)\n",
    "            candidate_entity = candidate_entity.to(local_rank, non_blocking=True)\n",
    "            entity_mask = entity_mask.to(local_rank, non_blocking=True)\n",
    "            clicked_entity = clicked_entity.to(local_rank, non_blocking=True)\n",
    "\n",
    "            scores = model.module.validation_process(subgraph, mappings, clicked_entity, candidate_emb, candidate_entity, entity_mask)\n",
    "            \n",
    "            tasks.append((labels.tolist(), scores))\n",
    "\n",
    "    with mp.Pool(processes=cfg.num_workers) as pool:\n",
    "        results = pool.map(cal_metric, tasks)\n",
    "    val_auc, val_mrr, val_ndcg5, val_ndcg10 = np.array(results).T\n",
    "\n",
    "    # barrier\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "    reduced_auc = reduce_mean(torch.tensor(np.nanmean(val_auc)).float().to(local_rank), cfg.gpu_num)\n",
    "    reduced_mrr = reduce_mean(torch.tensor(np.nanmean(val_mrr)).float().to(local_rank), cfg.gpu_num)\n",
    "    reduced_ndcg5 = reduce_mean(torch.tensor(np.nanmean(val_ndcg5)).float().to(local_rank), cfg.gpu_num)\n",
    "    reduced_ndcg10 = reduce_mean(torch.tensor(np.nanmean(val_ndcg10)).float().to(local_rank), cfg.gpu_num)\n",
    "\n",
    "    res = {\n",
    "        \"auc\": reduced_auc.item(),\n",
    "        \"mrr\": reduced_mrr.item(),\n",
    "        \"ndcg5\": reduced_ndcg5.item(),\n",
    "        \"ndcg10\": reduced_ndcg10.item(),\n",
    "    }\n",
    "    \n",
    "    return res\n",
    "\n",
    "def test(model, local_rank, cfg):\n",
    "    model.eval()\n",
    "    dataloader = load_data(cfg, mode='test', model=model, local_rank=local_rank)\n",
    "    val_dataset_len = len(dataloader)\n",
    "    print('test : len(dataloader)', len(dataloader))\n",
    "    tasks = []\n",
    "    print('gpus', cfg.gpu_num)\n",
    "    with torch.no_grad():\n",
    "        for cnt, (subgraph, mappings, clicked_entity, candidate_input, candidate_entity, entity_mask, labels) \\\n",
    "                in enumerate(tqdm(dataloader,\n",
    "                                  total=int(val_dataset_len),\n",
    "                                  desc=f\"[{local_rank}] Testing\")):\n",
    "            candidate_emb = torch.FloatTensor(np.array(candidate_input)).to(local_rank, non_blocking=True)\n",
    "            candidate_entity = candidate_entity.to(local_rank, non_blocking=True)\n",
    "            entity_mask = entity_mask.to(local_rank, non_blocking=True)\n",
    "            clicked_entity = clicked_entity.to(local_rank, non_blocking=True)\n",
    "\n",
    "            scores = model.module.validation_process(subgraph, mappings, clicked_entity, candidate_emb, candidate_entity, entity_mask)\n",
    "            \n",
    "            tasks.append((labels.tolist(), scores))\n",
    "\n",
    "    with mp.Pool(processes=cfg.num_workers) as pool:\n",
    "        results = pool.map(cal_metric, tasks)\n",
    "    val_auc, val_mrr, val_ndcg5, val_ndcg10 = np.array(results).T\n",
    "\n",
    "    # barrier\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "    reduced_auc = reduce_mean(torch.tensor(np.nanmean(val_auc)).float().to(local_rank), cfg.gpu_num)\n",
    "    reduced_mrr = reduce_mean(torch.tensor(np.nanmean(val_mrr)).float().to(local_rank), cfg.gpu_num)\n",
    "    reduced_ndcg5 = reduce_mean(torch.tensor(np.nanmean(val_ndcg5)).float().to(local_rank), cfg.gpu_num)\n",
    "    reduced_ndcg10 = reduce_mean(torch.tensor(np.nanmean(val_ndcg10)).float().to(local_rank), cfg.gpu_num)\n",
    "\n",
    "    res = {\n",
    "        \"auc\": reduced_auc.item(),\n",
    "        \"mrr\": reduced_mrr.item(),\n",
    "        \"ndcg5\": reduced_ndcg5.item(),\n",
    "        \"ndcg10\": reduced_ndcg10.item(),\n",
    "    }\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def main_worker(local_rank, cfg):\n",
    "    # -----------------------------------------Environment Initial\n",
    "    seed_everything(cfg.seed)\n",
    "    dist.init_process_group(backend='nccl',\n",
    "                            init_method='tcp://127.0.0.1:23456',\n",
    "                            world_size=cfg.gpu_num,\n",
    "                            rank=local_rank)\n",
    "\n",
    "    # -----------------------------------------Dataset & Model Load\n",
    "    num_training_steps = int(cfg.num_epochs * cfg.dataset.pos_count / (cfg.batch_size * cfg.accumulation_steps))\n",
    "    num_warmup_steps = int(num_training_steps * cfg.warmup_ratio + 1)\n",
    "    train_dataloader = load_data(cfg, mode='train', local_rank=local_rank)\n",
    "    model = load_model(cfg).to(local_rank)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.optimizer.lr)\n",
    "\n",
    "    lr_lambda = lambda step: 1.0 if step > num_warmup_steps else step / num_warmup_steps\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # ------------------------------------------Load Checkpoint & optimizer\n",
    "    if cfg.load_checkpoint:\n",
    "        file_path = Path(f\"{cfg.path.ckp_dir}/{cfg.model.model_name}_{cfg.dataset.dataset_name}_{cfg.load_mark}.pth\")\n",
    "        checkpoint = torch.load(file_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # After Distributed\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    print(\"dist!!!\")\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    # ------------------------------------------Main Start\n",
    "    early_stopping = EarlyStopping(cfg.early_stop_patience)\n",
    "\n",
    "    if local_rank == 0:\n",
    "        wandb.init(config=OmegaConf.to_container(cfg, resolve=True),\n",
    "                   project=cfg.logger.exp_name, name=cfg.logger.run_name)\n",
    "        print(model)\n",
    "\n",
    "    print(\"train\")\n",
    "    # for _ in tqdm(range(1, cfg.num_epochs + 1), desc=\"Epoch\"):\n",
    "    train(model, optimizer, scaler, scheduler, train_dataloader, local_rank, cfg, early_stopping)\n",
    "\n",
    "\n",
    "    if local_rank == 0:\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "@hydra.main(version_base=\"1.2\", config_path=os.path.join(get_root(), \"configs\"), config_name=\"EB\")\n",
    "def main(cfg: DictConfig):\n",
    "    seed_everything(cfg.seed)\n",
    "    cfg.gpu_num = torch.cuda.device_count()\n",
    "    # prepare_preprocessed_data(cfg)\n",
    "    mp.spawn(main_worker, nprocs=cfg.gpu_num, args=(cfg,))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
